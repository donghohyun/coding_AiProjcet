{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Conv2D, AveragePooling2D, Flatten, Dense, ZeroPadding2D\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet = Sequential([InputLayer(input_shape=(480,480,1)), \n",
    "                    ZeroPadding2D((2,2)),\n",
    "                    Conv2D(6,5, activation=\"tanh\"),\n",
    "                    AveragePooling2D(strides=2),\n",
    "                    Conv2D(16,5,activation=\"tanh\"),\n",
    "                    AveragePooling2D(strides=2),\n",
    "                    Conv2D(120,5, activation=\"tanh\"),\n",
    "                    Flatten(),\n",
    "                    Dense(84,activation=\"tanh\"),\n",
    "                    Dense(9,activation=\"softmax\")])\n",
    "\n",
    "# 모델 컴파일\n",
    "LeNet.compile(optimizer=\"SGD\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습을 위한 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def hangulFilePathImageRead ( filePath ) :\n",
    "\n",
    "    stream = open( filePath.encode(\"utf-8\") , \"rb\")\n",
    "    bytes = bytearray(stream.read())\n",
    "    numpyArray = np.asarray(bytes, dtype=np.uint8)\n",
    "\n",
    "    return cv2.imdecode(numpyArray , cv2.IMREAD_UNCHANGED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = []\n",
    "data_label = []\n",
    "cnt = 0\n",
    "label_dic = []\n",
    "\n",
    "# 디렉토리 경로\n",
    "directory_path = r'../img_data/processed_data'\n",
    "\n",
    "# 디렉토리 내의 파일 목록을 가져옵니다\n",
    "file_list = os.listdir(directory_path)\n",
    "\n",
    "for file_name in file_list:\n",
    "    img_file_list = os.listdir(f\"{directory_path}/{file_name}\")\n",
    "    label_dic.append(f\"{file_name} : {cnt}\")\n",
    "    \n",
    "    for img_i in img_file_list:\n",
    "        \n",
    "        file_path = f'{directory_path}/{file_name}/{img_i}'\n",
    "        img_ori = hangulFilePathImageRead(file_path)\n",
    "        img_ori = cv2.resize(img_ori, (480, 480))\n",
    "\n",
    "        height, width, channel = img_ori.shape\n",
    "\n",
    "        gray = cv2.cvtColor(img_ori, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        img_blurred = cv2.GaussianBlur(gray, ksize=(5, 5), sigmaX=0)\n",
    "        img_blur_thresh = cv2.adaptiveThreshold(\n",
    "            img_blurred,\n",
    "            maxValue=1,  # 변경된 부분: maxValue를 1로 설정\n",
    "            adaptiveMethod=cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            thresholdType=cv2.THRESH_BINARY_INV,\n",
    "            blockSize=19,\n",
    "            C=9\n",
    "        )\n",
    "\n",
    "        # 이미지 데이터를 0과 1로 표현\n",
    "        image_data = img_blur_thresh\n",
    "\n",
    "        data_input.append(image_data)\n",
    "        data_label.append(cnt)  # 해당 이미지의 레이블을 지정\n",
    "    cnt +=1\n",
    "\n",
    "# 데이터를 NumPy 배열로 변환\n",
    "data_input = np.array(data_input, dtype=np.int32)\n",
    "data_label = np.array(data_label, dtype=np.int32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data_input, data_label))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_input, train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 : 0',\n",
       " '2 : 1',\n",
       " '3 : 2',\n",
       " '4 : 3',\n",
       " '5 : 4',\n",
       " 'down : 5',\n",
       " 'left : 6',\n",
       " 'right : 7',\n",
       " 'up : 8']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 8:2 비율로 분할\n",
    "train_input, test_input, train_label, test_label = train_test_split(data_input, data_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "47/47 [==============================] - 63s 1s/step - loss: 0.2750 - accuracy: 0.9448\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 62s 1s/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 62s 1s/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 61s 1s/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 63s 1s/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 61s 1s/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 61s 1s/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 61s 1s/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 61s 1s/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 61s 1s/step - loss: 0.0015 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2421efee1d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LeNet.fit(train_input, train_label, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 3s 227ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Test Accuracy: 1.0\n",
      "Test Loss: 0.0024091526865959167\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = LeNet.evaluate(test_input, test_label)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "LeNet.save(\"../model/model_sep.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
